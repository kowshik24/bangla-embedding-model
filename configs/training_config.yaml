# Model Configuration
model: 
  # Using public multilingual models that don't require authentication
  base_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  teacher_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # For distillation
  max_seq_length: 256
  pooling_mode: "mean"

# Training Configuration
training:
  output_dir: "./outputs/bangla-embedding-model"
  num_epochs: 10
  batch_size: 32
  learning_rate: 2e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  fp16: true
  gradient_accumulation_steps: 2
  
# Loss Configuration
loss:
  primary:  "mnrl"  # MultipleNegativesRankingLoss
  use_matryoshka: true
  matryoshka_dims: [384, 256, 128, 64]  # MiniLM has 384 dimensions
  
# Data Configuration
data:
  train_data_path: "./data/processed/train.json"
  eval_data_path: "./data/processed/eval.json"
  parallel_data_path: "./data/parallel/en_bn_parallel.json"
  max_train_samples: null  # null for all
  hard_negatives_per_sample: 5

# Evaluation
evaluation:
  eval_steps: 500
  save_steps: 1000
  metric_for_best_model: "eval_loss"

# Logging
logging:
  use_wandb: true
  project_name: "bangla-embedding"
  run_name: "bangla-mpnet-v1"